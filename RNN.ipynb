{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safikhanSoofiyani/CS6910-Assignment-3/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks\n"
      ],
      "metadata": {
        "id": "U5HqFjcSVLy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWqqx4brVLH2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sc\n",
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input,InputLayer,Flatten,Activation,LSTM,SimpleRNN,GRU,TimeDistributed\n",
        "from tensorflow.keras.models import Sequential, Model,load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(hash(\"seriously you compete with me\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"i am mohammed safi\") % 2**32 - 1)\n",
        "tf.random.set_seed(hash(\"ur rahman khan\") % 2**32 - 1)"
      ],
      "metadata": {
        "id": "3sleHsMmYciG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras import backend as K\n",
        "K._get_available_gpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jr1oGJKYdAx",
        "outputId": "8d49b37a-8366-47a4-986c-3a841fa07d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4ZM-2CYg6R",
        "outputId": "4388a9dc-695c-4cf3-f750-3ac8f6df64a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "5X6ve7s2Yitv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY61iB5hYkyK",
        "outputId": "3ddfe4ce-e51d-48f8-b0eb-deafe3fb1b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English as source language and Telugu as target language\n",
        "source_language='en'\n",
        "target_language='te'"
      ],
      "metadata": {
        "id": "360zQMEyz92m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    path = r\"/content/drive/MyDrive/dakshina_dataset_v1.0/te/lexicons/\"\n",
        "\n",
        "    train_file_path = path + \"te.translit.sampled.train.tsv\"\n",
        "    test_file_path = path + \"te.translit.sampled.test.tsv\"\n",
        "    val_file_path = path + \"te.translit.sampled.dev.tsv\"\n",
        "\n",
        "    \n",
        "\n",
        "    #File containing training data\n",
        "    with open(train_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    #storing a list of list containing the two pair of words\n",
        "    train_pairs = []\n",
        "    for line in lines:\n",
        "        train_pairs.append(line.strip(\"\\n\").split(\"\\t\"))\n",
        "\n",
        "    #File containing test data\n",
        "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    #storing a list of list containing the two pair of words\n",
        "    test_pairs = []\n",
        "    for line in lines:\n",
        "        test_pairs.append(line.strip(\"\\n\").split(\"\\t\"))\n",
        "\n",
        "    #File containing validation data\n",
        "    with open(val_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    #storing a list of list containing the two pair of words\n",
        "    val_pairs = []\n",
        "    for line in lines:\n",
        "        val_pairs.append(line.strip(\"\\n\").split(\"\\t\"))\n",
        "\n",
        "    return train_pairs, val_pairs, test_pairs\n"
      ],
      "metadata": {
        "id": "hCq14cy_O6o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs, val_pairs, test_pairs = load_data()\n",
        "\n",
        "#print(train_pairs)"
      ],
      "metadata": {
        "id": "SzbX58kMBYmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(dataset):\n",
        "    \n",
        "    input_words = []\n",
        "    target_words = []\n",
        "    input_tokens = set()\n",
        "    target_tokens = set()\n",
        "\n",
        "    #iterate for each pair of words in the dataset\n",
        "    for pair in dataset:\n",
        "\n",
        "        #Add these words to the input and target list respectively\n",
        "        input_words.append(pair[1])\n",
        "        target_words.append(\"\\t\"+pair[0]+\"\\n\")\n",
        "\n",
        "        #adding the characters of the word into the tokens set to store the \n",
        "        #list of all the characters\n",
        "        for char in pair[1]:\n",
        "            input_tokens.add(char)\n",
        "        for char in pair[0]:\n",
        "            target_tokens.add(char)\n",
        "\n",
        "    input_tokens = sorted(list(input_tokens) + [\" \"])\n",
        "    target_tokens = sorted(list(target_tokens) + [\" \", '\\t', '\\n'])\n",
        "\n",
        "    num_enc_tokens = len(input_tokens)\n",
        "    num_tar_tokens = len(target_tokens)\n",
        "\n",
        "    max_inp_len = max([len(inp) for inp in input_words])\n",
        "    max_tar_len = max([len(inp) for inp in target_words])\n",
        "\n",
        "    print(\"Number of samples:\", len(dataset))\n",
        "    print(\"Number of unique input tokens:\", num_enc_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_tar_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_inp_len)\n",
        "    print(\"Max sequence length for outputs:\", max_tar_len)\n",
        "\n",
        "    inp_token_index = dict([(char,i) for i, char in enumerate(input_tokens)])\n",
        "    tar_token_index = dict([(char,i) for i, char in enumerate(target_tokens)])\n",
        "\n",
        "    reverse_input_char_index = dict((i, char) for char, i in inp_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in tar_token_index.items())\n",
        "\n",
        "    return input_words, target_words, inp_token_index, tar_token_index, reverse_input_char_index, reverse_target_char_index\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "raSU82DvPM2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_words, target_words, inp_token_index, tar_token_index, reverse_input_char_index, reverse_target_char_index = prepare_data(train_pairs)"
      ],
      "metadata": {
        "id": "xvfX482TSnBR",
        "outputId": "4baf4204-ba4e-4f58-e6af-f2ba2a984305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 58550\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp_token_index)"
      ],
      "metadata": {
        "id": "ohCs3FMQXi4r",
        "outputId": "b89d44c1-4bc8-4ac1-fae5-a936535779cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\t': 0, '\\n': 1, ' ': 2, 'ం': 3, 'ః': 4, 'అ': 5, 'ఆ': 6, 'ఇ': 7, 'ఈ': 8, 'ఉ': 9, 'ఊ': 10, 'ఋ': 11, 'ఎ': 12, 'ఏ': 13, 'ఐ': 14, 'ఒ': 15, 'ఓ': 16, 'ఔ': 17, 'క': 18, 'ఖ': 19, 'గ': 20, 'ఘ': 21, 'చ': 22, 'ఛ': 23, 'జ': 24, 'ఝ': 25, 'ఞ': 26, 'ట': 27, 'ఠ': 28, 'డ': 29, 'ఢ': 30, 'ణ': 31, 'త': 32, 'థ': 33, 'ద': 34, 'ధ': 35, 'న': 36, 'ప': 37, 'ఫ': 38, 'బ': 39, 'భ': 40, 'మ': 41, 'య': 42, 'ర': 43, 'ఱ': 44, 'ల': 45, 'ళ': 46, 'వ': 47, 'శ': 48, 'ష': 49, 'స': 50, 'హ': 51, 'ా': 52, 'ి': 53, 'ీ': 54, 'ు': 55, 'ూ': 56, 'ృ': 57, 'ె': 58, 'ే': 59, 'ై': 60, 'ొ': 61, 'ో': 62, 'ౌ': 63, '్': 64, '\\u200c': 65}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(input_words, target_words, inp_index, tar_index):\n",
        "\n",
        "    max_inp_len = max([len(inp) for inp in input_words])\n",
        "    max_tar_len = max([len(inp) for inp in target_words])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_words), max_inp_len, len(inp_index)), dtype = \"float32\" )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_words), max_tar_len, len(tar_index)), dtype = \"float32\" )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_words), max_tar_len, len(tar_index)), dtype = \"float32\" )\n",
        "    \n",
        "    for i, (inp, tar) in enumerate(zip(input_words, target_words)):\n",
        "        for t, char in enumerate(inp):\n",
        "            encoder_input_data[i,t,inp_index[char]] = 1.0\n",
        "        encoder_input_data[i, t+1:, inp_index[\" \"]] = 1.0\n",
        "\n",
        "        for t, char in enumerate(tar):\n",
        "            decoder_input_data[i,t,tar_index[char]] = 1.0\n",
        "\n",
        "            if t>0:\n",
        "                decoder_target_data[i,t-1,tar_index[char]] = 1.0\n",
        "\n",
        "        decoder_input_data[i, t+1:, tar_index[\" \"]] = 1.0\n",
        "        decoder_target_data[i, t:, tar_index[\" \"]] = 1.0\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n"
      ],
      "metadata": {
        "id": "TbpXbSoITQpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data, decoder_input_data, decoder_target_data =one_hot_encoding(input_words, target_words, inp_token_index, tar_token_index)"
      ],
      "metadata": {
        "id": "au3XJBhDXYfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_inp_len = max([len(inp) for inp in input_words])\n",
        "max_tar_len = max([len(inp) for inp in target_words])\n",
        "num_enc_tokens = len(inp_token_index)\n",
        "num_dec_tokens = len(tar_token_index)"
      ],
      "metadata": {
        "id": "T-MWnvYTMClt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(num_enc_tokens, num_tar_tokens, max_inp_len, max_tar_len, num_encoders, embed_size, dropout, num_decoders, hidden_layer_size):\n",
        "  # e_in : Encoder input\n",
        "  # e_out : Encoder output\n",
        "  # e_states: Encoder states\n",
        "  # d_in : Decoder input\n",
        "  # d_out : Decoder output\n",
        "  # d_dense : Dense layer for decoder\n",
        "\n",
        "  enc_in = Input(shape=(None,))\n",
        "  enc_out = Embedding(num_enc_tokens, embed_size, trainable=True)(enc_in)\n",
        "\n",
        "  enc_layers = []\n",
        "  enc_states = []\n",
        "  for i in range(1,num_encoders+1):\n",
        "    encoder = SimpleRNN(hidden_layer_size, return_state=True, return_sequences=True, dropout=dropout)\n",
        "    enc_layers.append(encoder)\n",
        "    enc_out, state_h = encoder(enc_out)\n",
        "    enc_states.append([state_h])\n",
        "    \n",
        "\n",
        "  dec_in = Input(shape=(None, ))\n",
        "  dec_out = Embedding(num_dec_tokens, embed_size, trainable=True)(dec_in)\n",
        "\n",
        "  dec_layers = []\n",
        "  for i in range(1,num_decoders+1):\n",
        "    decoder = SimpleRNN(hidden_layer_size, return_state=True, return_sequences=True, dropout=dropout)\n",
        "    dec_layers.append(decoder)\n",
        "    dec_out, _ = decoder(dec_out, initial_state = enc_states[0])\n",
        "  \n",
        "  \n",
        "  dec_dense = Dense(num_tar_tokens, activation=\"softmax\")\n",
        "  dec_out = dec_dense(dec_out)\n",
        "  model = Model([enc_in, dec_in], dec_out)\n",
        "\n",
        "  return model, enc_layers, dec_layers\n"
      ],
      "metadata": {
        "id": "08A5CzRKJGSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm(num_enc_tokens, num_tar_tokens, max_inp_len, max_tar_len, num_encoders, embed_size, dropout, num_decoders, hidden_layer_size):\n",
        "  # e_in : Encoder input\n",
        "  # e_out : Encoder output\n",
        "  # e_states: Encoder states\n",
        "  # d_in : Decoder input\n",
        "  # d_out : Decoder output\n",
        "  # d_dense : Dense layer for decoder\n",
        "  \n",
        "  enc_in = Input(shape=(None,))\n",
        "  enc_out = Embedding(num_enc_tokens, embed_size, trainable=True)(enc_in)\n",
        "\n",
        "  enc_layers = []\n",
        "  enc_states = []\n",
        "  for i in range(1,num_encoders+1):\n",
        "    encoder = LSTM(hidden_layer_size, return_state=True, return_sequences=True, dropout=dropout)\n",
        "    enc_layers.append(encoder)\n",
        "    enc_out, state_h, state_c = encoder(enc_out)\n",
        "    enc_states.append([state_h, state_c])\n",
        "    \n",
        "\n",
        "  dec_in = Input(shape=(None, ))\n",
        "  dec_out = Embedding(num_dec_tokens, embed_size, trainable=True)(dec_in)\n",
        "\n",
        "  dec_layers = []\n",
        "  for i in range(1,num_decoders+1):\n",
        "    decoder = LSTM(hidden_layer_size, return_state=True, return_sequences=True, dropout=dropout)\n",
        "    dec_layers.append(decoder)\n",
        "    dec_out, _, _ = decoder(dec_out, initial_state = enc_states[0])\n",
        "  \n",
        "  \n",
        "  dec_dense = Dense(num_tar_tokens, activation=\"softmax\")\n",
        "  dec_out = dec_dense(dec_out)\n",
        "  model = Model([enc_in, dec_in], dec_out)\n",
        "\n",
        "  return model, enc_layers, dec_layers\n"
      ],
      "metadata": {
        "id": "7cUrvk91JG7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gru(num_enc_tokens, num_tar_tokens, max_inp_len, max_tar_len, num_encoders, embed_size, dropout, num_decoders, hidden_layer_size):\n",
        "  # e_in : Encoder input\n",
        "  # e_out : Encoder output\n",
        "  # e_states: Encoder states\n",
        "  # d_in : Decoder input\n",
        "  # d_out : Decoder output\n",
        "  # d_dense : Dense layer for decoder\n",
        "  \n",
        " \n",
        "  enc_in = Input(shape=(None,))\n",
        "  enc_out = Embedding(num_enc_tokens, embed_size, trainable=True)(enc_in)\n",
        "\n",
        "  enc_layers = []\n",
        "  enc_states = []\n",
        "  for i in range(1,num_encoders+1):\n",
        "    encoder = GRU(hidden_layer_size, return_state=True, return_sequences=True, dropout=dropout)\n",
        "    enc_layers.append(encoder)\n",
        "    enc_out, state_h = encoder(enc_out)\n",
        "    enc_states.append([state_h])\n",
        "    \n",
        "\n",
        "  dec_in = Input(shape=(None, ))\n",
        "  dec_out = Embedding(num_dec_tokens, embed_size, trainable=True)(dec_in)\n",
        "\n",
        "  dec_layers = []\n",
        "  for i in range(1,num_decoders+1):\n",
        "    decoder = GRU(hidden_layer_size, return_state=True, return_sequences=True, dropout=dropout)\n",
        "    dec_layers.append(decoder)\n",
        "    dec_out, _ = decoder(dec_out, initial_state = enc_states[0])\n",
        "  \n",
        "  \n",
        "  dec_dense = Dense(num_tar_tokens, activation=\"softmax\")\n",
        "  dec_out = dec_dense(dec_out)\n",
        "  model = Model([enc_in, dec_in], dec_out)\n",
        "\n",
        "  return model, enc_layers, dec_layers\n"
      ],
      "metadata": {
        "id": "MNzll7fwJIv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transliteration(config,inp_charint,tgt_charint):\n",
        "  num_encoders=config[\"num_encoders\"]\n",
        "  cell=config[\"cell\"]\n",
        "  embed_size=config[\"embed_size\"]\n",
        "  dropout=config[\"dropout\"]\n",
        "  num_decoders=config[\"num_decoders\"]\n",
        "  hidden_layer_size=config[\"hidden_layer_size\"]\n",
        "\n",
        "  if cell == \"RNN\":\n",
        "    model, enc_layers, dec_layers=rnn(num_enc_tokens, num_tar_tokens, max_inp_len, max_tar_len, num_encoders, embed_size, dropout, num_decoders, hidden_layer_size)\n",
        "    return model\n",
        "  elif cell == \"LSTM\":\n",
        "    model, enc_layers, dec_layers=lstm(num_enc_tokens, num_tar_tokens, max_inp_len, max_tar_len, num_encoders, embed_size, dropout, num_decoders, hidden_layer_size)\n",
        "    return model\n",
        "  elif cell == \"GRU\":\n",
        "    model, enc_layers, dec_layers=gru(num_enc_tokens, num_tar_tokens, max_inp_len, max_tar_len, num_encoders, embed_size, dropout, num_decoders, hidden_layer_size)\n",
        "    return model\n",
        "  \n"
      ],
      "metadata": {
        "id": "3Q3ws80VJLE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HHWAyES7Bk3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, num_encoders, num_decoders, enc_layers, dec_layers, cell, hidden_layer_size):\n",
        "\n",
        "    enc_in = model.input[0]\n",
        "    EmbeddingLayer = model.layers[2]    \n",
        "    enc_out = EmbeddingLayer(enc_in)\n",
        "\n",
        "    enc_states = []\n",
        "    if cell == 'RNN':\n",
        "        for i in range(num_encoders):\n",
        "            enc_out, state_h = enc_layers[i](enc_out)\n",
        "            enc_states += [state_h] \n",
        "    elif cell == 'LSTM':\n",
        "        for i in range(num_encoders):\n",
        "            enc_out, state_h, state_c = enc_layers[i](enc_out)\n",
        "            enc_states += [state_h, state_c]   \n",
        "    elif cell == 'RNN':\n",
        "        for i in range(num_encoders):\n",
        "            enc_out, state_h = enc_layers[i](enc_out)\n",
        "            enc_states += [state_h] \n",
        "\n",
        "    encoder = Model(enc_in, enc_states + [enc_out])\n",
        "\n",
        "\n",
        "    dec_in = model.input[1]    \n",
        "    EmbeddingLayer = model.layers[3] \n",
        "    dec_out = EmbeddingLayer(dec_in)\n",
        "\n",
        "    dec_states = []\n",
        "    dec_initial_states = []\n",
        "    \n",
        "    if cell == 'RNN' :\n",
        "        for i in range(num_decoders):\n",
        "            dec_initial_states += [Input(shape=(hidden_layer_size,)]\n",
        "            dec_out, state_h = dec_layers[i](dec_out, initial_state = dec_initial_states[i])\n",
        "            dec_states += [state_h]\n",
        "    elif cell == \"LSTM\":\n",
        "        j=0\n",
        "        for i in range(num_decoders):\n",
        "            dec_initial_states += [Input(shape=(hidden_layer_size, )) , Input(shape=(hidden_layer_size, ))]\n",
        "            dec_out, state_h, state_c = dec_layers[i](dec_out, initial_state=dec_initial_states[i+j:i+j+2])\n",
        "            dec_states += [state_h , state_c]\n",
        "            j += 1\n",
        "    elif cell == \"GRU\":\n",
        "        for i in range(num_decoders):\n",
        "            dec_initial_states += [Input(shape=(hidden_layer_size,)]\n",
        "            dec_out, state_h = dec_layers[i](dec_out, initial_state = dec_initial_states[i])\n",
        "            dec_states += [state_h]\n",
        "\n",
        "    dec_dense = model.layers[4 + 2*num_encoders]\n",
        "    dec_out = dec_dense(dec_out)\n",
        "    decoder = Model([dec_in] + dec_initial_states, [dec_out] + dec_states)\n",
        "\n",
        "    return encoder, decoder "
      ],
      "metadata": {
        "id": "B41-EZurBk05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, encoder, decoder):\n",
        "\n",
        "    states_value = encoder.predict(input_seq)\n",
        "    states_value = states_value[:-1]\n",
        "\n",
        "    target_seq = np.zeros((1, 1)) \n",
        "    target_seq[0, 0] = tar_token_index[\"\\t\"]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        states_value = output_tokens[1:]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "wInV1LegGPjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eu8-0UBABkps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mm6C3VtBLWt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NVrYWDXrLWrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5zLAhf96LWpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cQS8pCqALWmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wueXj1QMLWkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7dQC1wynLWh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the project and the entity name for wandb sweeps\n",
        "project_name=\"\"\n",
        "entity_name=\"\"\n"
      ],
      "metadata": {
        "id": "OA-s381xhV9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  default_config={\n",
        "      \"cell\": \"RNN\",\n",
        "      \"embed_size\":256,\n",
        "      \"dropout\":0.2,\n",
        "      \"num_encoders\": 1,\n",
        "      \"num_decoders\": 1,\n",
        "      \"hidden_layer_size\":128,\n",
        "      \"epochs\": 1,\n",
        "      \"batch_size\": 64\n",
        "  }\n",
        "\n",
        "  wandb.init(config=default_config,project=project_name,entity=entity_name)\n",
        "  config=wandb.config\n",
        "  wandb.run.name=(\n",
        "      str(config.cell)\n",
        "      +\"_\"+str(config.embed_size)\n",
        "      +\"_\"+str(config.dropout)\n",
        "      +\"_\"+str(config.num_encoders)\n",
        "      +\"_\"+str(config.num_decoders)\n",
        "      +\"_\"+str(config.hidden_layer_size)\n",
        "      +\"_\"+str(config.epochs)\n",
        "      +\"_\"+str(config.batch_size)\n",
        "  )\n",
        "\n",
        "  wandb.run.save()\n",
        "\n",
        "  model=transliteration(config,inp_token_index,tgt_token_index)\n",
        "  \n",
        "  model.compile(\n",
        "      optimizer=\"adam\",\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "  earlystopping = EarlyStopping(\n",
        "          monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=2, mode=\"auto\"\n",
        "      )\n",
        "\n",
        "  model.fit(\n",
        "      [e_in,d_in],\n",
        "      d_t,\n",
        "      batch_size=config.batch_size,\n",
        "      epochs=config.epochs,\n",
        "      callbacks=[earlystopping,WandbCallback()]\n",
        "  )\n",
        "\n",
        "  wandb.finish()\n",
        "  "
      ],
      "metadata": {
        "id": "Mf9QCTKfhXu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sweeper(project_name,entity_name)\n",
        "  sweep_configuration={\n",
        "      \"method\": \"bayes\",\n",
        "      \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "      \"parameters\":{\n",
        "          \"cell\":{\"values\":[\"RNN\",\"GRU\",\"LSTM\"]},\n",
        "          \"embed_size\":{\"values\":[16,32,64,256]},\n",
        "          \"hidden_layer_size\":{\"values\":[16,32,64,256]},\n",
        "          \"num_encoders\":{\"values\":[1,2,3]},\n",
        "          \"num_decoders\":{\"values\":[1,2,3]},\n",
        "          \"dropout\":{\"values\":[0.2,0.3]},\n",
        "          \"epochs\":{\"values\":[5,10,15,20]},\n",
        "          \"batch_size\":{\"values\":[32,64]}\n",
        "      }\n",
        "  }\n",
        "\n",
        "  sweep_id=wandb.sweep(sweep_configuration,project=project_name,entity=entity_name)\n",
        "  wandb.agent(sweep_id,train)\n"
      ],
      "metadata": {
        "id": "b24xZ8FVhYcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the sweeps for hyperparameter tuning\n",
        "sweeper(project_name,entity_name)"
      ],
      "metadata": {
        "id": "LsBxEun_hc4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}